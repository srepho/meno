{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meno + BERTopic Integration Tutorial\n",
    "\n",
    "This notebook demonstrates how to integrate the [Meno](https://github.com/yourusername/meno) topic modeling toolkit with [BERTopic](https://github.com/MaartenGr/BERTopic) for advanced topic modeling.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook covers:\n",
    "\n",
    "1. Setting up the environment\n",
    "2. Loading and preprocessing data with Meno\n",
    "3. Topic modeling with BERTopic\n",
    "4. Hyperparameter optimization for BERTopic\n",
    "5. Combining Meno's workflow with BERTopic's topic modeling\n",
    "6. Visualizing and analyzing results\n",
    "\n",
    "By the end of this notebook, you'll understand how to leverage the strengths of both frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Add parent directory to path for Meno imports (if needed)\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# Import Meno\n",
    "from meno import MenoWorkflow\n",
    "from meno.modeling.bertopic_model import BERTopicModel\n",
    "\n",
    "# Import BERTopic components\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance\n",
    "from bertopic.dimensionality import UMAPReducer\n",
    "from bertopic.cluster import HDBSCANClusterer\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(\"output\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading and Preprocessing Data with Meno\n",
    "\n",
    "Meno provides powerful data preprocessing capabilities, including acronym detection and expansion, spelling correction, and text normalization. Let's use it to prepare our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample dataset from Hugging Face\n",
    "def load_data():\n",
    "    \"\"\"Load insurance dataset from Hugging Face.\"\"\"\n",
    "    print(\"Loading insurance dataset from Hugging Face...\")\n",
    "    dataset = load_dataset(\"soates/australian-insurance-pii-dataset-corrected\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"text\": dataset[\"train\"][\"original_text\"],\n",
    "        \"id\": dataset[\"train\"][\"id\"]\n",
    "    })\n",
    "    \n",
    "    # Take a sample for faster processing\n",
    "    df = df.sample(n=200, random_state=42)\n",
    "    print(f\"Loaded {len(df)} documents\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the data\n",
    "df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Meno Workflow\n",
    "workflow = MenoWorkflow()\n",
    "\n",
    "# Load data into workflow\n",
    "workflow.load_data(\n",
    "    data=df,\n",
    "    text_column=\"text\",\n",
    "    id_column=\"id\"\n",
    ")\n",
    "\n",
    "# Generate acronym report\n",
    "print(\"Detecting acronyms...\")\n",
    "acronym_report = workflow.generate_acronym_report(\n",
    "    min_length=2,\n",
    "    min_count=3,\n",
    "    output_path=str(output_dir / \"insurance_acronyms.html\"),\n",
    "    open_browser=False\n",
    ")\n",
    "\n",
    "# Define custom acronym mappings based on report\n",
    "acronym_mappings = {\n",
    "    \"PDS\": \"Product Disclosure Statement\",\n",
    "    \"CTP\": \"Compulsory Third Party\",\n",
    "    \"RSA\": \"Roadside Assistance\"\n",
    "}\n",
    "\n",
    "# Apply acronym expansions\n",
    "print(\"Expanding acronyms...\")\n",
    "workflow.expand_acronyms(custom_mappings=acronym_mappings)\n",
    "\n",
    "# Generate spelling report\n",
    "print(\"Detecting potential misspellings...\")\n",
    "spelling_report = workflow.generate_misspelling_report(\n",
    "    min_length=5,\n",
    "    min_count=2,\n",
    "    output_path=str(output_dir / \"insurance_misspellings.html\"),\n",
    "    open_browser=False\n",
    ")\n",
    "\n",
    "# Define custom spelling corrections\n",
    "spelling_corrections = {\n",
    "    \"recieved\": \"received\",\n",
    "    \"reciept\": \"receipt\"\n",
    "}\n",
    "\n",
    "# Apply spelling corrections\n",
    "print(\"Correcting spelling...\")\n",
    "workflow.correct_spelling(custom_corrections=spelling_corrections)\n",
    "\n",
    "# Preprocess text data\n",
    "print(\"Preprocessing documents...\")\n",
    "preprocessed_df = workflow.preprocess_documents(\n",
    "    lowercase=True,\n",
    "    remove_punctuation=True,\n",
    "    remove_stopwords=True,\n",
    "    additional_stopwords=[\"insurance\", \"policy\", \"claim\", \"customer\"]\n",
    ")\n",
    "\n",
    "# Display a sample preprocessed document\n",
    "print(\"\\nSample preprocessed document:\")\n",
    "print(preprocessed_df[\"processed_text\"].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Topic Modeling with BERTopic\n",
    "\n",
    "Now that we've preprocessed the data with Meno, let's use BERTopic for topic modeling. We'll demonstrate three approaches:\n",
    "\n",
    "1. Using Meno's BERTopicModel wrapper\n",
    "2. Using BERTopic directly with a simple configuration\n",
    "3. Using BERTopic with a custom pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Using Meno's BERTopicModel Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Meno's BERTopicModel\n",
    "meno_bertopic = BERTopicModel(\n",
    "    n_topics=10,\n",
    "    min_topic_size=5,\n",
    "    n_neighbors=15,\n",
    "    n_components=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "print(\"Fitting Meno BERTopicModel...\")\n",
    "meno_bertopic.fit(preprocessed_df[\"processed_text\"])\n",
    "\n",
    "# Display topic information\n",
    "print(\"\\nDiscovered topics:\")\n",
    "for topic_id, topic_name in meno_bertopic.topics.items():\n",
    "    if topic_id != -1:  # Skip outlier topic\n",
    "        topic_size = meno_bertopic.topic_sizes[topic_id]\n",
    "        print(f\"Topic {topic_id} ({topic_size} documents): {topic_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Using BERTopic Directly with Simple Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple BERTopic model\n",
    "simple_bertopic = BERTopic(\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    nr_topics=10,\n",
    "    min_topic_size=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "print(\"Fitting simple BERTopic model...\")\n",
    "topics, probs = simple_bertopic.fit_transform(preprocessed_df[\"processed_text\"].tolist())\n",
    "\n",
    "# Get topic information\n",
    "topic_info = simple_bertopic.get_topic_info()\n",
    "print(f\"\\nDiscovered {len(topic_info[topic_info['Topic'] != -1])} topics\")\n",
    "\n",
    "# Print top words for each topic\n",
    "print(\"\\nTop words per topic:\")\n",
    "for topic_id in sorted(simple_bertopic.get_topics().keys()):\n",
    "    if topic_id != -1:  # Skip outlier topic\n",
    "        topic_words = simple_bertopic.get_topic(topic_id)\n",
    "        words = [word for word, _ in topic_words[:5]]\n",
    "        print(f\"Topic {topic_id}: {', '.join(words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Using BERTopic with Custom Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom components\n",
    "print(\"Creating custom BERTopic pipeline...\")\n",
    "\n",
    "# Enhanced c-TF-IDF transformer\n",
    "ctfidf_model = ClassTfidfTransformer(\n",
    "    reduce_frequent_words=True,  # Reduce impact of frequent terms\n",
    "    bm25_weighting=True          # Use BM25 weighting for better results\n",
    ")\n",
    "\n",
    "# Dimensionality reduction with UMAP\n",
    "umap_model = UMAPReducer(\n",
    "    n_neighbors=15,              # Balance local vs global structure\n",
    "    n_components=5,              # Intermediate dimensionality\n",
    "    min_dist=0.1,                # How tightly to pack points\n",
    "    metric=\"cosine\",             # Distance metric\n",
    "    low_memory=True              # Memory optimization\n",
    ")\n",
    "\n",
    "# Clustering with HDBSCAN\n",
    "hdbscan_model = HDBSCANClusterer(\n",
    "    min_cluster_size=5,          # Minimum size of clusters (smaller for demonstration)\n",
    "    min_samples=3,               # Sample size for core points\n",
    "    metric=\"euclidean\",          # Distance metric\n",
    "    prediction_data=True,        # Store data for predicting new points\n",
    "    cluster_selection_method=\"eom\"  # Excess of mass method\n",
    ")\n",
    "\n",
    "# Topic representation model - combining two approaches\n",
    "keybert_model = KeyBERTInspired()                # Better keywords\n",
    "mmr_model = MaximalMarginalRelevance(diversity=0.3)  # Diversity in representation\n",
    "\n",
    "# Create BERTopic model with custom pipeline\n",
    "custom_bertopic = BERTopic(\n",
    "    # Main embedding model\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    \n",
    "    # Custom components\n",
    "    ctfidf_model=ctfidf_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    representation_model=[keybert_model, mmr_model],  # Pipeline of representations\n",
    "    \n",
    "    # Model parameters\n",
    "    nr_topics=10,\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "print(\"Fitting custom BERTopic model...\")\n",
    "topics, probs = custom_bertopic.fit_transform(preprocessed_df[\"processed_text\"].tolist())\n",
    "\n",
    "# Get topic information\n",
    "topic_info = custom_bertopic.get_topic_info()\n",
    "print(f\"\\nDiscovered {len(topic_info[topic_info['Topic'] != -1])} topics\")\n",
    "\n",
    "# Print top words for each topic\n",
    "print(\"\\nTop words per topic:\")\n",
    "for topic_id in sorted(custom_bertopic.get_topics().keys()):\n",
    "    if topic_id != -1:  # Skip outlier topic\n",
    "        topic_words = custom_bertopic.get_topic(topic_id)\n",
    "        words = [word for word, _ in topic_words[:5]]\n",
    "        print(f\"Topic {topic_id}: {', '.join(words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Optimization for BERTopic\n",
    "\n",
    "BERTopic has several components that can be tuned. Let's create a function to optimize the most important hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bertopic_model(model, documents):\n",
    "    \"\"\"Evaluate a BERTopic model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : BERTopic\n",
    "        The fitted BERTopic model\n",
    "    documents : list\n",
    "        List of document texts\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    # Get model information\n",
    "    topic_info = model.get_topic_info()\n",
    "    topics, _ = model.transform(documents)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    n_topics = len(topic_info[topic_info['Topic'] != -1])\n",
    "    outlier_percentage = topic_info.iloc[0]['Count'] / len(documents) * 100\n",
    "    avg_topic_size = topic_info[topic_info['Topic'] != -1]['Count'].mean()\n",
    "    \n",
    "    # Calculate topic diversity based on word overlap\n",
    "    all_topic_words = []\n",
    "    for topic_id in model.get_topics():\n",
    "        if topic_id != -1:  # Skip outlier topic\n",
    "            words = [word for word, _ in model.get_topic(topic_id)[:10]]\n",
    "            all_topic_words.append(set(words))\n",
    "    \n",
    "    # Calculate average Jaccard similarity (lower is better / more diverse)\n",
    "    diversity_score = 0\n",
    "    comparisons = 0\n",
    "    for i in range(len(all_topic_words)):\n",
    "        for j in range(i+1, len(all_topic_words)):\n",
    "            intersection = len(all_topic_words[i].intersection(all_topic_words[j]))\n",
    "            union = len(all_topic_words[i].union(all_topic_words[j]))\n",
    "            if union > 0:\n",
    "                similarity = intersection / union\n",
    "                diversity_score += similarity\n",
    "                comparisons += 1\n",
    "    \n",
    "    word_diversity = 1 - (diversity_score / max(1, comparisons))\n",
    "    \n",
    "    return {\n",
    "        \"n_topics\": n_topics,\n",
    "        \"outlier_percentage\": outlier_percentage,\n",
    "        \"avg_topic_size\": avg_topic_size,\n",
    "        \"word_diversity\": word_diversity,\n",
    "        \"combined_score\": n_topics * word_diversity * (100 - outlier_percentage) / 100\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_bertopic_hyperparameters(documents, n_trials=3):\n",
    "    \"\"\"Optimize BERTopic hyperparameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    documents : list\n",
    "        List of document texts\n",
    "    n_trials : int, optional\n",
    "        Number of trials to run, by default 3\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (best_params, best_model, best_score)\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    \n",
    "    # Define hyperparameter grid\n",
    "    param_grid = {\n",
    "        \"n_neighbors\": [5, 15, 30],\n",
    "        \"n_components\": [5, 10, 15],\n",
    "        \"min_cluster_size\": [5, 10, 15],\n",
    "        \"diversity\": [0.1, 0.3, 0.5]\n",
    "    }\n",
    "    \n",
    "    # For the notebook demo, use a smaller grid\n",
    "    # In a real scenario, you would use the full grid or even more parameters\n",
    "    if n_trials < 5:  # Use a reduced grid for quick demonstration\n",
    "        param_grid = {\n",
    "            \"n_neighbors\": [15],\n",
    "            \"n_components\": [5],\n",
    "            \"min_cluster_size\": [5, 10],\n",
    "            \"diversity\": [0.3]\n",
    "        }\n",
    "    \n",
    "    # Generate parameter combinations\n",
    "    param_names = list(param_grid.keys())\n",
    "    param_values = list(param_grid.values())\n",
    "    param_combinations = list(itertools.product(*param_values))\n",
    "    \n",
    "    # If too many combinations, select a random subset\n",
    "    import random\n",
    "    if len(param_combinations) > n_trials:\n",
    "        random.seed(42)\n",
    "        param_combinations = random.sample(param_combinations, n_trials)\n",
    "    \n",
    "    # Evaluate each combination\n",
    "    best_score = -float('inf')\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    print(f\"Running {len(param_combinations)} trials...\")\n",
    "    \n",
    "    for i, values in enumerate(param_combinations):\n",
    "        params = dict(zip(param_names, values))\n",
    "        print(f\"\\nTrial {i+1}/{len(param_combinations)}: {params}\")\n",
    "        \n",
    "        # Create components with these parameters\n",
    "        umap_model = UMAPReducer(\n",
    "            n_neighbors=params[\"n_neighbors\"],\n",
    "            n_components=params[\"n_components\"],\n",
    "            min_dist=0.1,\n",
    "            metric=\"cosine\",\n",
    "            low_memory=True\n",
    "        )\n",
    "        \n",
    "        hdbscan_model = HDBSCANClusterer(\n",
    "            min_cluster_size=params[\"min_cluster_size\"],\n",
    "            min_samples=params[\"min_cluster_size\"] // 2,\n",
    "            metric=\"euclidean\",\n",
    "            prediction_data=True,\n",
    "            cluster_selection_method=\"eom\"\n",
    "        )\n",
    "        \n",
    "        # Topic representation with diversity parameter\n",
    "        keybert_model = KeyBERTInspired()\n",
    "        mmr_model = MaximalMarginalRelevance(diversity=params[\"diversity\"])\n",
    "        \n",
    "        # Create BERTopic model\n",
    "        model = BERTopic(\n",
    "            embedding_model=\"all-MiniLM-L6-v2\",\n",
    "            umap_model=umap_model,\n",
    "            hdbscan_model=hdbscan_model,\n",
    "            representation_model=[keybert_model, mmr_model],\n",
    "            calculate_probabilities=True,\n",
    "            verbose=False  # Reduce output during optimization\n",
    "        )\n",
    "        \n",
    "        start_time = time()\n",
    "        \n",
    "        # Fit the model\n",
    "        try:\n",
    "            model.fit_transform(documents)\n",
    "            \n",
    "            # Evaluate the model\n",
    "            metrics = evaluate_bertopic_model(model, documents)\n",
    "            print(f\"Metrics: {metrics}\")\n",
    "            \n",
    "            # Check if this is the best model so far\n",
    "            if metrics[\"combined_score\"] > best_score:\n",
    "                best_score = metrics[\"combined_score\"]\n",
    "                best_params = params\n",
    "                best_model = model\n",
    "                print(f\"New best model! Score: {best_score:.2f}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error with parameters {params}: {str(e)}\")\n",
    "            \n",
    "        print(f\"Trial completed in {time() - start_time:.1f} seconds\")\n",
    "    \n",
    "    return best_params, best_model, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the hyperparameter optimization\n",
    "print(\"Optimizing BERTopic hyperparameters...\")\n",
    "best_params, best_model, best_score = optimize_bertopic_hyperparameters(\n",
    "    preprocessed_df[\"processed_text\"].tolist(),\n",
    "    n_trials=2  # Use a small number for demonstration\n",
    ")\n",
    "\n",
    "print(f\"\\nBest hyperparameters: {best_params}\")\n",
    "print(f\"Best score: {best_score:.2f}\")\n",
    "\n",
    "# Get topic information from the best model\n",
    "topic_info = best_model.get_topic_info()\n",
    "print(f\"\\nNumber of topics in best model: {len(topic_info[topic_info['Topic'] != -1])}\")\n",
    "\n",
    "# Print top words for each topic\n",
    "print(\"\\nTop words per topic in best model:\")\n",
    "for topic_id in sorted(best_model.get_topics().keys()):\n",
    "    if topic_id != -1:  # Skip outlier topic\n",
    "        topic_words = best_model.get_topic(topic_id)\n",
    "        words = [word for word, _ in topic_words[:5]]\n",
    "        print(f\"Topic {topic_id}: {', '.join(words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Combining Meno's Workflow with BERTopic\n",
    "\n",
    "Now let's demonstrate how to integrate the optimized BERTopic model with Meno's workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign topics from the best BERTopic model to the documents\n",
    "topics, probs = best_model.transform(preprocessed_df[\"processed_text\"].tolist())\n",
    "\n",
    "# Create a DataFrame with topic assignments\n",
    "topic_df = pd.DataFrame({\n",
    "    \"topic\": [f\"Topic_{t}\" if t >= 0 else \"Outlier\" for t in topics],\n",
    "    \"topic_probability\": [max(prob) if max(prob) > 0 else 0 for prob in probs]\n",
    "})\n",
    "\n",
    "# Update the Meno workflow with BERTopic results\n",
    "workflow.set_topic_assignments(topic_df)\n",
    "\n",
    "# Generate Meno visualizations\n",
    "print(\"Generating Meno visualizations...\")\n",
    "\n",
    "# Topic embedding visualization\n",
    "embedding_viz = workflow.visualize_topics(plot_type=\"embeddings\")\n",
    "embedding_viz.write_html(str(output_dir / \"topic_embeddings.html\"))\n",
    "\n",
    "# Topic distribution visualization\n",
    "distribution_viz = workflow.visualize_topics(plot_type=\"distribution\")\n",
    "distribution_viz.write_html(str(output_dir / \"topic_distribution.html\"))\n",
    "\n",
    "# Generate comprehensive report\n",
    "print(\"\\nGenerating comprehensive report...\")\n",
    "report_path = workflow.generate_comprehensive_report(\n",
    "    output_path=str(output_dir / \"meno_bertopic_report.html\"),\n",
    "    title=\"Meno + BERTopic Integration Results\",\n",
    "    include_interactive=True,\n",
    "    include_raw_data=True,\n",
    "    open_browser=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizing and Analyzing Results\n",
    "\n",
    "Let's create some visualizations to analyze our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate BERTopic visualizations\n",
    "print(\"Generating BERTopic visualizations...\")\n",
    "\n",
    "# Topic similarity visualization\n",
    "best_model.visualize_topics().write_html(str(output_dir / \"bertopic_similarity.html\"))\n",
    "\n",
    "# Topic hierarchy\n",
    "best_model.visualize_hierarchy().write_html(str(output_dir / \"bertopic_hierarchy.html\"))\n",
    "\n",
    "# Topic barchart\n",
    "best_model.visualize_barchart(top_n_topics=10).write_html(str(output_dir / \"bertopic_barchart.html\"))\n",
    "\n",
    "# List all generated files\n",
    "print(\"\\nGenerated files:\")\n",
    "for file in output_dir.glob(\"*.html\"):\n",
    "    print(f\"- {file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to:\n",
    "\n",
    "1. Use Meno's preprocessing capabilities to clean and prepare text data\n",
    "2. Apply BERTopic for topic modeling, using both simple and advanced configurations\n",
    "3. Optimize BERTopic hyperparameters for better topic quality\n",
    "4. Integrate Meno's workflow system with BERTopic's topic modeling\n",
    "5. Generate comprehensive visualizations and reports\n",
    "\n",
    "This integration combines Meno's strength in data preprocessing and workflow management with BERTopic's state-of-the-art topic modeling capabilities, resulting in a powerful solution for text analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}